{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAQIXHttp1H7tvyF9NuLtM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edudati/LLMsGeneOntology/blob/main/functionDescriptionUniProt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUajw82IZiTm",
        "outputId": "f1a75d40-e5d9-4973-8fe8-38d7394b83cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# --- Parte 1: Imports e configurações ---\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Instalar e configurar NLTK\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parte 2: Funções ---\n",
        "\n",
        "# 2.1 Pegar o Uniprot Id a partir do nome do gene\n",
        "def get_uniprot_id(gene):\n",
        "    url = f\"https://rest.uniprot.org/uniprotkb/search?query=gene:{gene}+AND+organism_id:9606+AND+reviewed:true&format=json&size=1\"\n",
        "    response = requests.get(url)\n",
        "    time.sleep(0.2)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data.get(\"results\"):\n",
        "            return data[\"results\"][0][\"primaryAccession\"]\n",
        "    return \"Not found\"\n"
      ],
      "metadata": {
        "id": "TMFqMxpnqu_E"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parte 2: Funções ---\n",
        "\n",
        "# 2.2 Pegar a função do gene descrita no Uniprot\n",
        "def get_function_description(uniprot_id):\n",
        "    if uniprot_id == \"Not found\":\n",
        "        return \"Not found\"\n",
        "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\"\n",
        "    response = requests.get(url)\n",
        "    time.sleep(0.2)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        comments = data.get(\"comments\", [])\n",
        "        for comment in comments:\n",
        "            if comment.get(\"commentType\") == \"FUNCTION\":\n",
        "                texts = comment.get(\"texts\", [])\n",
        "                return \" \".join(text[\"value\"] for text in texts if \"value\" in text)\n",
        "    return \"Not found\""
      ],
      "metadata": {
        "id": "BjiZ6k2JqsQW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parte 2: Funções ---\n",
        "\n",
        "# 2.3 Pegar os GO Ids a partir do Uniprot Id\n",
        "def get_go_ids(uniprot_id):\n",
        "    if uniprot_id in [\"Not found\", \"Error\"]:\n",
        "        return \"Not found\"\n",
        "    url = f\"https://www.ebi.ac.uk/QuickGO/services/annotation/search?geneProductId=UniProtKB:{uniprot_id}&limit=100\"\n",
        "    response = requests.get(url)\n",
        "    time.sleep(0.2)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        go_ids = set()\n",
        "        for result in data.get(\"results\", []):\n",
        "            go_id = result.get(\"goId\")\n",
        "            if go_id:\n",
        "                go_ids.add(go_id)\n",
        "        return list(go_ids)\n",
        "    return \"Error\"\n"
      ],
      "metadata": {
        "id": "BcLxHomQqpFY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parte 2: Funções ---\n",
        "\n",
        "# 2.4 Pegar as definições para os GO Ids\n",
        "def get_go_definition(go_id):\n",
        "    url = f\"https://www.ebi.ac.uk/QuickGO/services/ontology/go/terms/{go_id}\"\n",
        "    response = requests.get(url)\n",
        "    time.sleep(0.2)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if results:\n",
        "            name = results[0].get(\"name\", \"\")\n",
        "            definition = results[0].get(\"definition\", {}).get(\"text\", \"\")\n",
        "            aspect = results[0].get(\"aspect\", \"\")\n",
        "            return {\"goId\": go_id, \"name\": name, \"definition\": definition, \"aspect\": aspect}\n",
        "    return {\"goId\": go_id, \"name\": \"\", \"definition\": \"\", \"aspect\": \"\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "3y1V0zsGrP8c"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parte 2: Funções ---\n",
        "\n",
        "# 2.5 Extrair palavas-chave de um texto\n",
        "def extract_keywords(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    words = tokenizer.tokenize(text.lower())\n",
        "    return list(set(word for word in words if word.isalpha() and word not in stop_words))\n"
      ],
      "metadata": {
        "id": "dPbXT0zTrSzc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parte 3: Execução ---\n",
        "\n",
        "genes = ['FXN', 'ATM', 'INVALIDGENE']  # Substitua pela lista real\n",
        "\n",
        "# Para planilha 1\n",
        "summary_rows = []\n",
        "\n",
        "# Para planilha 2\n",
        "go_rows = []\n",
        "\n",
        "for gene in genes:\n",
        "    uniprot_id = get_uniprot_id(gene)\n",
        "    function_desc = get_function_description(uniprot_id)\n",
        "\n",
        "    # Extrair palavras-chave\n",
        "    if isinstance(function_desc, str):\n",
        "        words = tokenizer.tokenize(function_desc.lower())\n",
        "        function_keywords = list(set(\n",
        "            word for word in words if word.isalpha() and word not in stop_words\n",
        "        ))\n",
        "    else:\n",
        "        function_keywords = []\n",
        "\n",
        "    go_ids = get_go_ids(uniprot_id)\n",
        "\n",
        "    # Adicionar à planilha 1\n",
        "    summary_rows.append({\n",
        "        \"gene_name\": gene,\n",
        "        \"uniProt_id\": uniprot_id,\n",
        "        \"function_description\": function_desc,\n",
        "        \"function_keywords\": \", \".join(function_keywords),\n",
        "        \"go_ids_list\": go_ids if isinstance(go_ids, list) else []\n",
        "    })\n",
        "\n",
        "    # Adicionar à planilha 2\n",
        "    if isinstance(go_ids, list):\n",
        "        for go_id in go_ids:\n",
        "            go = get_go_definition(go_id)\n",
        "            go_rows.append({\n",
        "                \"gene_name\": gene,\n",
        "                \"uniProt_id\": uniprot_id,\n",
        "                \"goId\": go[\"goId\"],\n",
        "                \"goAspect\": go[\"aspect\"],\n",
        "                \"goName\": go[\"name\"],\n",
        "                \"goDefinition\": go[\"definition\"]\n",
        "            })\n",
        "    else:\n",
        "        go_rows.append({\n",
        "            \"gene_name\": gene,\n",
        "            \"uniProt_id\": uniprot_id,\n",
        "            \"goId\": \"Not found\",\n",
        "            \"goAspect\": \"\",\n",
        "            \"goName\": \"\",\n",
        "            \"goDefinition\": \"\"\n",
        "        })\n"
      ],
      "metadata": {
        "id": "NRzw0mXOqkNf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parte 4: Exportar ---\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "go_df = pd.DataFrame(go_rows)\n",
        "\n",
        "summary_df.to_excel(\"output-01_summary_by_gene.xlsx\", index=False)\n",
        "go_df.to_excel(\"output-02_go_terms_expanded.xlsx\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"output-01_summary_by_gene.xlsx\")\n",
        "files.download(\"output-02_go_terms_expanded.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LkYbId-LqgWB",
        "outputId": "6a8fdd06-89c8-4b86-bf24-59cc50785154"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c47b8138-c0f4-49d2-a356-97b39a459ab4\", \"output-05_summary_by_gene.xlsx\", 8433)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bc0a987c-56e9-4515-8929-fc20b801014d\", \"output-06_go_terms_expanded.xlsx\", 12582)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parte 5: Análise linguística ---\n",
        "\n",
        "# Instalar bibliotecas necessárias\n",
        "!pip install nltk spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Carregar modelo do spaCy para NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Inicializadores\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Simular df_func se necessário\n",
        "# df_func = pd.read_excel(\"output-01_Function-Description-with-Keywords.xlsx\")\n",
        "\n",
        "# Funções de processamento\n",
        "def lemmatise_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    return list(set(lemmatizer.lemmatize(w) for w in tokens if w.isalpha() and w not in stop_words))\n",
        "\n",
        "def get_pos_tags(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "def get_bigrams(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "    return [\" \".join(bg) for bg in ngrams(tokens, 2)]\n",
        "\n",
        "def get_named_entities(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    doc = nlp(text)\n",
        "    return list(set(ent.text for ent in doc.ents))\n",
        "\n",
        "def extract_pos(text):\n",
        "    if not isinstance(text, str):\n",
        "        return [], []\n",
        "    tagged = pos_tag(tokenizer.tokenize(text.lower()))\n",
        "    verbs = [word for word, tag in tagged if tag.startswith(\"VB\")]\n",
        "    adjs = [word for word, tag in tagged if tag.startswith(\"JJ\")]\n",
        "    return list(set(verbs)), list(set(adjs))\n",
        "\n",
        "def extract_pos_spacy(text):\n",
        "    if not isinstance(text, str):\n",
        "        return [], []\n",
        "    doc = nlp(text)\n",
        "    verbs = list(set([token.text for token in doc if token.pos_ == \"VERB\"]))\n",
        "    adjs = list(set([token.text for token in doc if token.pos_ == \"ADJ\"]))\n",
        "    return verbs, adjs\n",
        "\n",
        "\n",
        "# Aplicar ao DataFrame\n",
        "linguistic_rows = []\n",
        "\n",
        "for _, row in df_func.iterrows():\n",
        "    desc = row[\"function_description\"]\n",
        "    tokens = tokenizer.tokenize(desc.lower()) if isinstance(desc, str) else []\n",
        "    content_words = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "    verbs, adjs = extract_pos_spacy(desc)\n",
        "\n",
        "    linguistic_rows.append({\n",
        "        \"gene_name\": row[\"gene_name\"],\n",
        "        \"uniProt_id\": row[\"uniProt_id\"],\n",
        "        \"function_description\": desc,\n",
        "        \"keywords\": list(set(content_words)),\n",
        "        \"lemmatised_words\": list(set(lemmatizer.lemmatize(w) for w in content_words)),\n",
        "        \"pos_tags\": list(set(pos_tag(tokens))) if tokens else [],\n",
        "        \"verbs\": verbs,\n",
        "        \"adjectives\": adjs,\n",
        "        \"bigrams\": list(set(\" \".join(bg) for bg in ngrams(content_words, 2))),\n",
        "        \"named_entities\": list(set(ent.text for ent in nlp(desc).ents)) if isinstance(desc, str) else []\n",
        "    })\n",
        "\n",
        "\n",
        "linguistic_df = pd.DataFrame(linguistic_rows)\n",
        "\n",
        "# Exportar para Excel\n",
        "linguistic_df.to_excel(\"output-03_function_linguistic_analysis.xlsx\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"output-03_function_linguistic_analysis.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GXVed8KA1VvV",
        "outputId": "1b040795-73a4-4bba-f59e-32929b1dca34"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_615e38b0-5360-46f9-9ab7-29ef553313aa\", \"output-07_function_linguistic_analysis.xlsx\", 12357)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}